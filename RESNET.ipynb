{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "print(tf.VERSION)\n",
    "\n",
    "conv = tf.layers.conv2d\n",
    "bn = tf.layers.batch_normalization\n",
    "relu = tf.nn.relu\n",
    "he = tf.keras.initializers.he_normal\n",
    "l2 = tf.contrib.layers.l2_regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_block(inputs,lr,filters,training):\n",
    "    conv1 = conv(inputs=inputs,filters=filters,kernel_size=[3,3],padding='same',activation=None,kernel_initializer=he,kernel_regularizer=l2(lr),name='conv1')\n",
    "    bn1 = bn(inputs=conv1,training=training,name='bn1')\n",
    "    relu1 = tf.nn.relu(bn1,name='relu1')\n",
    "    conv2 = conv(inputs=relu1,filters=filters,kernel_size=[3,3],padding='same',activation=None,kernel_initializer=he,kernel_regularizer=l2(lr),name='conv2')\n",
    "    bn2 = bn(inputs=conv2,training=training,name='bn2')\n",
    "    skip = tf.add(inputs,bn2,name='skip')\n",
    "    result = tf.nn.relu(skip,name='relu2')\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs,lr,filters,training):\n",
    "    conv_skip = conv(inputs=inputs,filters=filters,kernel_size=[2,2],strides=[2,2],padding='valid',activation=None,kernel_initializer=he,kernel_regularizer=l2(lr),name='conv-skip')\n",
    "    bn_skip = bn(inputs=conv_skip,training=training,name='bn_skip')\n",
    "    conv1 = conv(inputs=inputs,filters=filters,kernel_size=[3,3],strides=[2,2],padding='valid',activation=None,kernel_initializer=he,kernel_regularizer=l2(lr),name='conv1')\n",
    "    bn1 = bn(inputs=conv1,training=training,name='bn1')\n",
    "    relu1 = tf.nn.relu(bn1,name='relu1')\n",
    "    conv2 = conv(inputs=relu1,filters=filters,kernel_size=[3,3],padding='same',activation=None,kernel_initializer=he,kernel_regularizer=l2(lr),name='conv2')\n",
    "    bn2 = bn(inputs=conv2,training=training,name='bn2')\n",
    "    skip_join = tf.add(bn_skip,bn2,name='skip_join')\n",
    "    result = tf.nn.relu(skip_join,name='relu2')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train =  np.asarray(x_train,dtype=np.float32)\n",
    "x_test =  np.asarray(x_test,dtype=np.float32)\n",
    "y_train =  np.asarray(y_train,dtype=np.int32)\n",
    "y_test =  np.asarray(y_test,dtype=np.int32)\n",
    "#x_train /= 255.0;\n",
    "#x_test /= 255.0;\n",
    "mean = np.mean(x_train)\n",
    "std = np.std(x_train)\n",
    "x_train = (x_train - mean)/(std + 10e-7)\n",
    "x_test = (x_test - mean)/(std + 10e-7)\n",
    "\n",
    "feature_cols = [tf.feature_column.numeric_column('x')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(images, labels,resize=None,horizontal_flip=False,vertical_flip=False,rotate=0, crop_probability=0,crop_min_percent=0.6,crop_max_percent=1.,mixup=0):  \n",
    "    if resize is not None:\n",
    "        images = tf.image.resize_bilinear(images, resize)\n",
    "  \n",
    "  # My experiments showed that casting on GPU improves training performance\n",
    "    if images.dtype != tf.float32:\n",
    "        images = tf.image.convert_image_dtype(images, dtype=tf.float32)\n",
    "        images = tf.subtract(images, 0.5)\n",
    "        images = tf.multiply(images, 2.0)\n",
    "        labels = tf.to_float(labels)\n",
    "\n",
    "    with tf.name_scope('augmentation'):\n",
    "        shp = tf.shape(images)\n",
    "        batch_size, height, width = shp[0], shp[1], shp[2]\n",
    "        width = tf.cast(width, tf.float32)\n",
    "        height = tf.cast(height, tf.float32)\n",
    "        transforms = []\n",
    "        identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)\n",
    "        if horizontal_flip:\n",
    "            coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n",
    "            flip_transform = tf.convert_to_tensor(\n",
    "          [-1., 0., width, 0., 1., 0., 0., 0.], dtype=tf.float32)\n",
    "            transforms.append(\n",
    "                tf.where(coin,\n",
    "                   tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]),\n",
    "                   tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])))\n",
    "       \n",
    "        if vertical_flip:\n",
    "            coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n",
    "            flip_transform = tf.convert_to_tensor(\n",
    "                [1, 0, 0, 0, -1, height, 0, 0], dtype=tf.float32)\n",
    "            transforms.append(\n",
    "                  tf.where(coin,\n",
    "                   tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]),\n",
    "                   tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])))\n",
    "\n",
    "        if rotate > 0:\n",
    "            angle_rad = rotate / 180 * np.pi\n",
    "            angles = tf.random_uniform([batch_size], -angle_rad, angle_rad)\n",
    "            transforms.append(\n",
    "            tf.contrib.image.angles_to_projective_transforms(\n",
    "              angles, height, width))\n",
    "\n",
    "        if crop_probability > 0:\n",
    "            crop_pct = tf.random_uniform([batch_size], crop_min_percent,\n",
    "                                   crop_max_percent)\n",
    "            left = tf.random_uniform([batch_size], 0, width * (1 - crop_pct))\n",
    "            top = tf.random_uniform([batch_size], 0, height * (1 - crop_pct))\n",
    "            crop_transform = tf.stack([\n",
    "                crop_pct,\n",
    "                tf.zeros([batch_size]), top,\n",
    "                tf.zeros([batch_size]), crop_pct, left,\n",
    "                tf.zeros([batch_size]),tf.zeros([batch_size])], 1)\n",
    "\n",
    "            coin = tf.less(\n",
    "              tf.random_uniform([batch_size], 0, 1.0), crop_probability)\n",
    "            transforms.append(\n",
    "              tf.where(coin, crop_transform,\n",
    "                   tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])))\n",
    "\n",
    "        if transforms:\n",
    "              images = tf.contrib.image.transform(\n",
    "              images,\n",
    "              tf.contrib.image.compose_transforms(*transforms),\n",
    "              interpolation='BILINEAR') # or 'NEAREST'\n",
    "\n",
    "        def cshift(values): # Circular shift in batch dimension\n",
    "              return tf.concat([values[-1:, ...], values[:-1, ...]], 0)\n",
    "\n",
    "        if mixup > 0:\n",
    "            mixup = 1.0 * mixup\n",
    "            beta = tf.distributions.Beta(mixup, mixup)\n",
    "            lam = beta.sample(batch_size)\n",
    "            ll = tf.expand_dims(tf.expand_dims(tf.expand_dims(lam, -1), -1), -1)\n",
    "            images = ll * images + (1 - ll) * cshift(images)\n",
    "            labels = lam * labels + (1 - lam) * cshift(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(images,labels,batch_size,num):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'x' : images},labels))\n",
    "    def _aug(images,labels):\n",
    "        images,labels = augment(images['x'],labels,horizontal_flip=True,rotate=15,crop_probability=0.8)\n",
    "        return {'x' : images},labels\n",
    "    augmented = dataset.map(_aug)\n",
    "    dataset.concatenate(augmented)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_model(features,labels,mode,params):\n",
    "    lmd = params['regulariser']\n",
    "    inputs = tf.feature_column.input_layer(features,params['feature_cols'])\n",
    "    inputs = tf.reshape(inputs,[-1,32,32,3])\n",
    "    inputs = conv(inputs=inputs,filters=16,kernel_size=[3,3],padding='same',activation=None,kernel_initializer=he,kernel_regularizer=l2(lmd),name='first_conv')\n",
    "    inputs = bn(inputs=inputs,training=(mode==tf.estimator.ModeKeys.TRAIN),name='first_bn')\n",
    "    inputs = tf.nn.relu(inputs,name='first_relu')\n",
    "    \n",
    "    for block,filters,num in params['resnet']:\n",
    "        for i in range(num):\n",
    "            with tf.variable_scope(block + str(filters) + \"-\" + str(i+1)):\n",
    "                if block == 'id':\n",
    "                    inputs = id_block(inputs,lmd,filters,mode==tf.estimator.ModeKeys.TRAIN)\n",
    "                else:\n",
    "                    inputs = conv_block(inputs,lmd,filters,mode==tf.estimator.ModeKeys.TRAIN)\n",
    "        print(\"Dimensions now \" + str(inputs.shape))\n",
    "    \n",
    "    inputs = tf.reduce_mean(inputs,axis=[1,2],name='global_avg_pool')\n",
    "    print(\"Average pooling dimensions\" + str(inputs.shape))\n",
    "    inputs = tf.reshape(inputs,[-1,params['flat_size']])\n",
    "    logits = tf.layers.dense(inputs=inputs,units=params['n_classes'],activation=tf.nn.relu,kernel_initializer=he,kernel_regularizer=l2(lmd))\n",
    "    \n",
    "    \n",
    "    \n",
    "    predictions = {\n",
    "        'classes' : tf.argmax(logits,axis=1),\n",
    "        'probabilities' : tf.nn.softmax(logits)\n",
    "    }\n",
    "    \n",
    "    eval_ops = {\n",
    "        'accuracy' : tf.metrics.accuracy(labels=labels,predictions=predictions['classes'])\n",
    "    }\n",
    "    \n",
    "    tf.summary.scalar(eval_ops['accuracy'])\n",
    "    \n",
    "    if mode==tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(predictions=predictions,mode=mode)\n",
    "    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,logits=logits)\n",
    "    \n",
    "    if mode==tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,loss=loss,train_op=train_op)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode,loss=loss,eval_metric_ops=eval_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'RESNET', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7efbe04a8a90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "resnet56 = [('id',16,9),('conv',32,1),('id',32,9),('conv',64,1),('id',64,1)]\n",
    "classifier = tf.estimator.Estimator(model_fn=resnet_model,model_dir='RESNET',params={\n",
    "    'feature_cols' : feature_cols,\n",
    "    'regulariser' : 0.0005,\n",
    "    'learning_rate' : 0.001,\n",
    "    'resnet' : resnet56,\n",
    "    'flat_size' : 64\n",
    "}) \n",
    "classifier.train(input_fn=lambda:training_data(x_train,y_train,250,x_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
